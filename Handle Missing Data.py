# -*- coding: utf-8 -*-
"""DS440 Handle Missing Data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UuqXwwApICNb9YGfcGK0SiBZB4n_ZEWG
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sqlite3
import gc
# import preprocessing classes
from sklearn.preprocessing import OrdinalEncoder, LabelEncoder

# feature selection
from sklearn.feature_selection import chi2, mutual_info_regression
from sklearn.feature_selection import SelectKBest, chi2

# import models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR

# import train test split
from sklearn.model_selection import train_test_split

# feature importance
from sklearn.inspection import permutation_importance

# import metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error

# functions to save data
from joblib import dump, load

# read data
## mount google drive
from google.colab import drive
drive.mount("/content/drive/")
# %cd '/content/drive/MyDrive/DS440/dataset/Data/'
!pwd

## read in files
dir = '/content/drive/MyDrive/DS440/dataset/Data/'
cnx = sqlite3.connect(dir + 'wildfire.sqlite')
cursor = cnx.cursor()
cursor.execute("SELECT name FROM sqlite_master WHERE type='Fires';")
tables = cursor.fetchall()
Fires = pd.read_sql_query("SELECT * FROM 'Fires'", cnx).set_index('FOD_ID')
gc.collect()
# Drop unnecessary column
Fires.drop(columns=['FPA_ID','SOURCE_SYSTEM_TYPE','SOURCE_SYSTEM','NWCG_REPORTING_AGENCY','NWCG_REPORTING_UNIT_ID',
                    'NWCG_REPORTING_UNIT_NAME','SOURCE_REPORTING_UNIT','SOURCE_REPORTING_UNIT_NAME','LOCAL_FIRE_REPORT_ID',
                    'LOCAL_INCIDENT_ID','FIRE_CODE','FIRE_NAME','ICS_209_PLUS_INCIDENT_JOIN_ID','ICS_209_PLUS_COMPLEX_JOIN_ID',
                    'MTBS_ID','MTBS_FIRE_NAME','COMPLEX_NAME','OWNER_DESCR','NWCG_CAUSE_CLASSIFICATION',
                    'NWCG_CAUSE_AGE_CATEGORY','CONT_TIME','CONT_DATE'],inplace=True)

Fires.head()

"""# Data Preprocessing
## convert all features into usable format
## Use random sample from originial dataset due to high computational cost
## Take 1% from each FIRE_SIZE_CLASS to ensure that fire size distribution is uniform after random sampling
## Purpose : 
1. To handle big data
2. To debug and to see that the code actually works
"""

Fires_A = Fires[Fires['FIRE_SIZE_CLASS'] == 'A'].sample(frac=0.03,replace=False)
Fires_B = Fires[Fires['FIRE_SIZE_CLASS'] == 'B'].sample(frac=0.03,replace=False)
Fires_C = Fires[Fires['FIRE_SIZE_CLASS'] == 'C'].sample(frac=0.03,replace=False)
Fires_D = Fires[Fires['FIRE_SIZE_CLASS'] == 'D'].sample(frac=0.03,replace=False)
Fires_E = Fires[Fires['FIRE_SIZE_CLASS'] == 'E'].sample(frac=0.03,replace=False)
Fires_F = Fires[Fires['FIRE_SIZE_CLASS'] == 'F'].sample(frac=0.03,replace=False)
Fires_G = Fires[Fires['FIRE_SIZE_CLASS'] == 'G'].sample(frac=0.03,replace=False)
Fires_AB = pd.concat([Fires_A, Fires_B], ignore_index=True)
Fires_CD = pd.concat([Fires_C, Fires_D], ignore_index=True)
Fires_EF = pd.concat([Fires_E, Fires_F], ignore_index=True)
Fires_EFG = pd.concat([Fires_EF, Fires_G], ignore_index=True)
Fires_ABCD = pd.concat([Fires_AB, Fires_CD], ignore_index=True)
Fires = pd.concat([Fires_ABCD, Fires_EFG], ignore_index=True)
Fires

"""# Gabriel's Coding"""

# extract month from discovery date
def get_first_element(date_list):
  return date_list[0]

Fires['DISCOVERY_DATE'] = Fires['DISCOVERY_DATE'].astype("string")
Fires['DISCOVERY_LIST'] = Fires['DISCOVERY_DATE'].str.split(pat='/')
Fires['DISCOVERY_MONTH'] = Fires['DISCOVERY_LIST'].map(get_first_element)
Fires['DISCOVERY_MONTH'] = Fires['DISCOVERY_MONTH'].astype("int64")

# encode causes into integers
Fires['NWCG_GENERAL_CAUSE'] = Fires['NWCG_GENERAL_CAUSE'].astype("string")
cause_enc = LabelEncoder()
Fires['NWCG_CAUSE_CLASSIFICATION_ORD'] = cause_enc.fit_transform(Fires['NWCG_GENERAL_CAUSE'])


# figure out number of days to contain the fire
Fires['DAYS_TO_CONT'] = Fires['CONT_DOY'] - Fires['DISCOVERY_DOY']
##### if it was contained the next year, need to do something about that
def convert_negative_days(day):
  if day < 0:
    return 365 + day
  else:
    return day

Fires['DAYS_TO_CONT'] = Fires['DAYS_TO_CONT'].map(convert_negative_days)


# encode state categories into integers
Fires['STATE'] = Fires['STATE'].astype("string")
state_enc = LabelEncoder()
Fires['STATE_ORD'] = state_enc.fit_transform(Fires['STATE'])

# what to do about counties? Use fips code (must be an int)
Fires['FIPS_CODE'] = Fires['FIPS_CODE'].astype("string")
Fires['FIPS_CODE'].fillna('1000000',inplace=True)
Fires['FIPS_CODE'] = Fires['FIPS_CODE'].astype("int64")

# fill NA with mean of day to containment (mean = 0.92)
Fires['DAYS_TO_CONT'].fillna(value=Fires['DAYS_TO_CONT'].mean(),inplace=True)


# drop date and list
Fires.drop(columns=['DISCOVERY_DATE','DISCOVERY_LIST','NWCG_GENERAL_CAUSE',
                    'CONT_DOY','STATE',
                    'COUNTY'],inplace=True)
gc.collect()

Fires.head()

Fires.isna().sum()

"""#** Code by Heejun Son**
### Need to handle missing values in DISCOVERY_TIME column

"""

Fires['FIPS_NAME'].fillna('1000000',inplace=True)

Fires = Fires[Fires['FIPS_NAME'] != '1000000']
Fires.drop(columns=['FIPS_CODE','DAYS_TO_CONT'],inplace=True)
# encode state categories into integers
FIPS_enc = LabelEncoder()
Fires['FIPS_NAME'] = FIPS_enc.fit_transform(Fires['FIPS_NAME'])

#Drop all the NaN values
Fires_Drop = Fires.sample(frac=1,random_state=0).dropna()
#Replace NaN with mean
Fires_Mean = Fires.sample(frac=1,random_state=0)
Fires_Mean['DISCOVERY_TIME'] = Fires_Mean[['DISCOVERY_TIME']].fillna(Fires_Mean[['DISCOVERY_TIME']].mean().iloc[0])
#Replace NaN with median
Fires_Med = Fires.sample(frac=1,random_state=0)
Fires_Med['DISCOVERY_TIME'] = Fires_Med[['DISCOVERY_TIME']].fillna(Fires_Med[['DISCOVERY_TIME']].median().iloc[0])
#Replace NaN with mode
Fires_Freq = Fires.sample(frac=1,random_state=0)
Fires_Freq['DISCOVERY_TIME'] = Fires_Freq[['DISCOVERY_TIME']].fillna(Fires_Freq[['DISCOVERY_TIME']].mode().iloc[0])

#Replace NaN with values predicted by KNN
from sklearn.impute import KNNImputer
Fires_KNN = Fires.sample(frac=1,random_state=0)
Input = Fires_KNN[['FIRE_YEAR','DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
imputer = KNNImputer(n_neighbors=10)

result = imputer.fit_transform(Input)
check = pd.DataFrame(result)
New_Discovery_Time = check[[1]].values.tolist()

Fires_KNN['Assumption'] = New_Discovery_Time
# Convert object to float64
Fires_KNN['Assumption'] = Fires_KNN['Assumption'].astype("string")
Fires_KNN['Assumption'] = Fires_KNN['Assumption'].str.strip('[]')
Fires_KNN['Assumption'] = Fires_KNN['Assumption'].astype("float64")
# Drop DISCOVERY_TIME column that contains NaN
Fires_KNN.drop(columns=['DISCOVERY_TIME'],inplace=True)
# Change column name from Assumption to DISCOVERY_TIME
Fires_KNN['DISCOVERY_TIME'] = Fires_KNN['Assumption']
Fires_KNN.drop(columns=['Assumption'],inplace=True)
Fires_KNN

#### split data into training and testing
X_drop_train = Fires_Drop[Fires_Drop['FIRE_YEAR'] < 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
X_drop_test = Fires_Drop[Fires_Drop['FIRE_YEAR'] < 2018]['FIRE_SIZE']
y_drop_train = Fires_Drop[Fires_Drop['FIRE_YEAR'] == 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
y_drop_test = Fires_Drop[Fires_Drop['FIRE_YEAR'] == 2018]['FIRE_SIZE']
######################################################################################################################################################################
X_mean_train = Fires_Mean[Fires_Mean['FIRE_YEAR'] < 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
X_mean_test = Fires_Mean[Fires_Mean['FIRE_YEAR'] < 2018]['FIRE_SIZE']
y_mean_train = Fires_Mean[Fires_Mean['FIRE_YEAR'] == 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
y_mean_test = Fires_Mean[Fires_Mean['FIRE_YEAR'] == 2018]['FIRE_SIZE']
######################################################################################################################################################################
X_med_train = Fires_Med[Fires_Med['FIRE_YEAR'] < 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
X_med_test = Fires_Med[Fires_Med['FIRE_YEAR'] < 2018]['FIRE_SIZE']
y_med_train = Fires_Med[Fires_Med['FIRE_YEAR'] == 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
y_med_test = Fires_Med[Fires_Med['FIRE_YEAR'] == 2018]['FIRE_SIZE']
######################################################################################################################################################################
X_mod_train = Fires_Freq[Fires_Freq['FIRE_YEAR'] < 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
X_mod_test = Fires_Freq[Fires_Freq['FIRE_YEAR'] < 2018]['FIRE_SIZE']
y_mod_train = Fires_Freq[Fires_Freq['FIRE_YEAR'] == 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
y_mod_test = Fires_Freq[Fires_Freq['FIRE_YEAR'] == 2018]['FIRE_SIZE']
######################################################################################################################################################################
X_KNN_train = Fires_KNN[Fires_KNN['FIRE_YEAR'] < 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
X_KNN_test = Fires_KNN[Fires_KNN['FIRE_YEAR'] < 2018]['FIRE_SIZE']
y_KNN_train = Fires_KNN[Fires_KNN['FIRE_YEAR'] == 2018][['DISCOVERY_DOY','DISCOVERY_TIME','LATITUDE','LONGITUDE','FIPS_NAME','DISCOVERY_MONTH','NWCG_CAUSE_CLASSIFICATION_ORD','STATE_ORD']]
y_KNN_test = Fires_KNN[Fires_KNN['FIRE_YEAR'] == 2018]['FIRE_SIZE']

"""- Drop"""

#### list of models to train
# xgboost, linear regression (with and without regularization), random forest, naive bayes, nearest neighbors
# k-means, svm
model_list = [LinearRegression, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,
               KNeighborsRegressor]
model_scores_drop = dict()    
model_drop_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE'])
for model in model_list:
  regressor_drop = model().fit(X_drop_train,X_drop_test)
  y_drop_pred = regressor_drop.predict(y_drop_train)
  model_drop_mse = mean_squared_error(y_drop_pred,y_drop_test)
  model_drop_mae = mean_absolute_error(y_drop_pred,y_drop_test)
  model_scores_drop[model] = (regressor_drop,y_drop_test,y_drop_pred)
  model_drop_df = model_drop_df.append({'Model':model,'MSE':model_drop_mse,'MAE':model_drop_mae}, ignore_index=True)
  model_drop_df = model_drop_df.sort_values(by='MSE', ascending= True)
model_drop_df

"""- Mean"""

#### list of models to train
# xgboost, linear regression (with and without regularization), random forest, naive bayes, nearest neighbors
# k-means, svm
model_list = [LinearRegression, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,
               KNeighborsRegressor]
model_scores_mean = dict()    
model_mean_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE'])
for model in model_list:
  regressor_mean = model().fit(X_mean_train,X_mean_test)
  y_mean_pred = regressor_mean.predict(y_mean_train)
  model_mean_mse = mean_squared_error(y_mean_pred,y_mean_test)
  model_mean_mae = mean_absolute_error(y_mean_pred,y_mean_test)
  model_scores_mean[model] = (regressor_mean,y_mean_test,y_mean_pred)
  model_mean_df = model_mean_df.append({'Model':model,'MSE':model_mean_mse,'MAE':model_mean_mae}, ignore_index=True)
  model_mean_df = model_mean_df.sort_values(by='MSE', ascending= True)
model_mean_df

"""- Median"""

#### list of models to train
# xgboost, linear regression (with and without regularization), random forest, naive bayes, nearest neighbors
# k-means, svm
model_list = [LinearRegression, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,
               KNeighborsRegressor]
model_scores_med = dict()    
model_med_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE'])
for model in model_list:
  regressor_med = model().fit(X_med_train,X_med_test)
  y_med_pred = regressor_med.predict(y_med_train)
  model_med_mse = mean_squared_error(y_med_pred,y_med_test)
  model_med_mae = mean_absolute_error(y_med_pred,y_med_test)
  model_scores_med[model] = (regressor_med,y_med_test,y_med_pred)
  model_med_df = model_med_df.append({'Model':model,'MSE':model_med_mse,'MAE':model_med_mae}, ignore_index=True)
  model_med_df = model_med_df.sort_values(by='MSE', ascending= True)
model_med_df

"""- Mode"""

#### list of models to train
# xgboost, linear regression (with and without regularization), random forest, naive bayes, nearest neighbors
# k-means, svm
model_list = [LinearRegression, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,
               KNeighborsRegressor]
model_scores_mod = dict()    
model_mod_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE'])
for model in model_list:
  regressor_mod = model().fit(X_mod_train,X_mod_test)
  y_mod_pred = regressor_mod.predict(y_mod_train)
  model_mod_mse = mean_squared_error(y_mod_pred,y_mod_test)
  model_mod_mae = mean_absolute_error(y_mod_pred,y_mod_test)
  model_scores_mod[model] = (regressor_mod,y_mod_test,y_mod_pred)
  model_mod_df = model_mod_df.append({'Model':model,'MSE':model_mod_mse,'MAE':model_mod_mae}, ignore_index=True)
  model_mod_df = model_mod_df.sort_values(by='MSE', ascending= True)
model_mod_df

"""- KNN"""

#### list of models to train
# xgboost, linear regression (with and without regularization), random forest, naive bayes, nearest neighbors
# k-means, svm
model_list = [LinearRegression, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,
               KNeighborsRegressor]
model_scores_KNN = dict()    
model_KNN_df = pd.DataFrame(columns=['Model', 'MSE', 'MAE'])
for model in model_list:
  regressor_KNN = model().fit(X_KNN_train,X_KNN_test)
  y_KNN_pred = regressor_KNN.predict(y_KNN_train)
  model_KNN_mse = mean_squared_error(y_KNN_pred,y_KNN_test)
  model_KNN_mae = mean_absolute_error(y_KNN_pred,y_KNN_test)
  model_scores_KNN[model] = (regressor_KNN,y_KNN_test,y_KNN_pred)
  model_KNN_df = model_KNN_df.append({'Model':model,'MSE':model_KNN_mse,'MAE':model_KNN_mae}, ignore_index=True)
  model_KNN_df = model_KNN_df.sort_values(by='MSE', ascending= True)
model_KNN_df